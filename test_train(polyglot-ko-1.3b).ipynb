{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squarelike/sharegpt_deepl_ko_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬에 사용된 진리 값과 연결은 실제 개념에 기반하지 않기 때문에 이 특정 비결정적 행렬로 정확하게 모델링할 수 있는 실제 상황을 찾기는 어렵습니다.\n",
      "\n",
      "일반적으로 비결정적 행렬 의미론은 불확실성 또는 여러 가지 가능한 결과가 있는 상황을 모델링하는 데 사용할 수 있습니다. 예를 들어 한 사람이 파티에 갈지 여부를 결정하는 상황을 생각해 보겠습니다. 이 사람이 취할 수 있는 행동에는 파티에 가거나 집에 머무르는 두 가지가 있습니다. 이 사람은 어떤 행동을 취해야 할지 불확실할 수 있으므로 \"파티에 가자\"라는 명제의 진리 값은 2(가능하지만 확실하지 않음을 나타냄)이고 \"집에 머물러라\"라는 명제의 진리 값은 3(가능하지만 확실하지 않음을 나타냄)일 수 있습니다.\n",
      "\n",
      "그러나 이는 가능한 한 가지 예일 뿐이며 상황의 구체적인 세부 사항은 비결정적 행렬에 사용된 진리 값과 연결어의 특정 해석에 따라 달라질 수 있습니다.\n",
      "\n",
      "도움이 되셨기를 바랍니다! 추가 질문이 있으시면 언제든지 알려주시기 바랍니다.\n",
      "\n",
      "It is difficult to find a real-world situation that would be accurately modeled by this specific non-deterministic matrix, as the truth-values and connectives used in the matrix are not based on any real-world concepts.\n",
      "\n",
      "In general, non-deterministic matrix semantics can be used to model situations where there is uncertainty or multiple possible outcomes. For example, consider a situation where a person is deciding whether or not to go to a party. There are two possible actions they can take: go to the party or stay home. The person may be uncertain about which action to take, and so the truth-value of the proposition \"go to the party\" may be 2 (indicating that it is possible but not certain) while the truth-value of the proposition \"stay home\" may be 3 (indicating that it is possible but not certain).\n",
      "\n",
      "However, this is just one possible example and the specific details of the situation would depend on the particular interpretation of the truth-values and connectives used in the non-deterministic matrix.\n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "print(data['train'][20000]['korean'])\n",
    "print()\n",
    "print(data['train'][20000]['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 형태를 prompt에 맞추기\n",
    "# korean과 english로 나뉜 데이터를 text로 통합\n",
    "# datasets의 map함수 사용법 https://huggingface.co/docs/datasets/v2.14.4/en/process#map\n",
    "\n",
    "def makedata(x):\n",
    "    return {'text': f\"### 영어: {x['english']}</끝>\\n### 한국어: {x['korean']}</끝>\"}\n",
    "\n",
    "data = data.map(makedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 100,\n",
       " 'korean': \"회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 '선택 및 마스크' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 '색조/채도' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.\",\n",
       " 'english': 'If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.',\n",
       " 'text': '### 영어: If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.</끝>\\n### 한국어: 회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 \\'선택 및 마스크\\' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 \\'색조/채도\\' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.</끝>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f492f6859d44484a09449ea419d7849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>',\n",
       " 'input_ids': [6,\n",
       "  6,\n",
       "  6,\n",
       "  3029,\n",
       "  29,\n",
       "  1795,\n",
       "  17291,\n",
       "  72,\n",
       "  224,\n",
       "  17081,\n",
       "  12063,\n",
       "  8759,\n",
       "  11594,\n",
       "  9883,\n",
       "  7743,\n",
       "  86,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  7789,\n",
       "  26618,\n",
       "  22253,\n",
       "  5941,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  29,\n",
       "  202,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9244,\n",
       "  22757,\n",
       "  3883,\n",
       "  29650,\n",
       "  10903,\n",
       "  28078,\n",
       "  7789,\n",
       "  20705,\n",
       "  11518,\n",
       "  4454,\n",
       "  14445,\n",
       "  2479,\n",
       "  5862,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  21924,\n",
       "  7993,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  202,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  224,\n",
       "  3328,\n",
       "  86,\n",
       "  82,\n",
       "  224,\n",
       "  29598,\n",
       "  3628,\n",
       "  8812,\n",
       "  22757,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  13652,\n",
       "  12063,\n",
       "  89,\n",
       "  82,\n",
       "  8614,\n",
       "  224,\n",
       "  10414,\n",
       "  89,\n",
       "  4541,\n",
       "  13384,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  3432,\n",
       "  3354,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  25614,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  2218,\n",
       "  2479,\n",
       "  12131,\n",
       "  92,\n",
       "  15,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9059,\n",
       "  26477,\n",
       "  12509,\n",
       "  12063,\n",
       "  224,\n",
       "  4569,\n",
       "  79,\n",
       "  21473,\n",
       "  8759,\n",
       "  7094,\n",
       "  29828,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  8945,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  28428,\n",
       "  13652,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  224,\n",
       "  21840,\n",
       "  2423,\n",
       "  3864,\n",
       "  7948,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  73,\n",
       "  7094,\n",
       "  29828,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  21924,\n",
       "  87,\n",
       "  9347,\n",
       "  81,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  202,\n",
       "  18,\n",
       "  18,\n",
       "  23375,\n",
       "  3354,\n",
       "  224,\n",
       "  29598,\n",
       "  29650,\n",
       "  14445,\n",
       "  20904,\n",
       "  72,\n",
       "  22253,\n",
       "  5941,\n",
       "  29,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33,\n",
       "  202,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  8611,\n",
       "  29,\n",
       "  307,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  365,\n",
       "  327,\n",
       "  272,\n",
       "  1870,\n",
       "  1384,\n",
       "  1891,\n",
       "  270,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  20,\n",
       "  17,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  954,\n",
       "  1553,\n",
       "  1033,\n",
       "  286,\n",
       "  11567,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  15407,\n",
       "  21396,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  1203,\n",
       "  13473,\n",
       "  409,\n",
       "  998,\n",
       "  417,\n",
       "  276,\n",
       "  2129,\n",
       "  283,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  286,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  2477,\n",
       "  482,\n",
       "  1062,\n",
       "  293,\n",
       "  550,\n",
       "  954,\n",
       "  823,\n",
       "  2248,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  661,\n",
       "  417,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  2086,\n",
       "  339,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  20637,\n",
       "  4296,\n",
       "  1979,\n",
       "  284,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  316,\n",
       "  746,\n",
       "  16776,\n",
       "  847,\n",
       "  419,\n",
       "  301,\n",
       "  3607,\n",
       "  482,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  953,\n",
       "  419,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  224,\n",
       "  8924,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  348,\n",
       "  3962,\n",
       "  2489,\n",
       "  485,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  96,\n",
       "  18,\n",
       "  18,\n",
       "  1462,\n",
       "  359,\n",
       "  2192,\n",
       "  1203,\n",
       "  284,\n",
       "  1934,\n",
       "  29,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainble%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 729403392 || trainble%: 0.21563705588032142\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenNum_korean = 8611\n",
    "tokenNum_english = 3029\n",
    "tokenNum_colon = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class maskTrainer(Trainer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # print(inputs['labels'][1])\n",
    "    for x in range(len(inputs['labels'])):\n",
    "      if (inputs['labels'][x][3] == tokenNum_korean):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_english).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      elif (inputs['labels'][x][3] == tokenNum_english):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_korean).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      inputs['labels'][x][:maskindex[0]+2] = -100\n",
    "\n",
    "    # print(inputs['labels'][1])\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    return (loss,outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66842' max='66842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66842/66842 11:18:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.784900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.739600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.640300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.635700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.628100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.588300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.602600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.578300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.599300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.595200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.563500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.587700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>0.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.556300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>0.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39900</td>\n",
       "      <td>0.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41100</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>0.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41700</td>\n",
       "      <td>0.530100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42300</td>\n",
       "      <td>0.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>0.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42900</td>\n",
       "      <td>0.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43800</td>\n",
       "      <td>0.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44100</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44400</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44700</td>\n",
       "      <td>0.514500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45300</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45900</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46200</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46800</td>\n",
       "      <td>0.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47100</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47400</td>\n",
       "      <td>0.543400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47700</td>\n",
       "      <td>0.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48300</td>\n",
       "      <td>0.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48600</td>\n",
       "      <td>0.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48900</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49200</td>\n",
       "      <td>0.528600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49800</td>\n",
       "      <td>0.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50100</td>\n",
       "      <td>0.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>0.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50700</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.530100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51300</td>\n",
       "      <td>0.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51600</td>\n",
       "      <td>0.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51900</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52200</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.529500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53100</td>\n",
       "      <td>0.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53400</td>\n",
       "      <td>0.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53700</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.529900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54300</td>\n",
       "      <td>0.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54600</td>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54900</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>0.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55800</td>\n",
       "      <td>0.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56100</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56400</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56700</td>\n",
       "      <td>0.527300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57300</td>\n",
       "      <td>0.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57600</td>\n",
       "      <td>0.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57900</td>\n",
       "      <td>0.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58200</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58800</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59100</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59400</td>\n",
       "      <td>0.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59700</td>\n",
       "      <td>0.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60300</td>\n",
       "      <td>0.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60600</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60900</td>\n",
       "      <td>0.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61200</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61800</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62100</td>\n",
       "      <td>0.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62400</td>\n",
       "      <td>0.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62700</td>\n",
       "      <td>0.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63300</td>\n",
       "      <td>0.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63600</td>\n",
       "      <td>0.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63900</td>\n",
       "      <td>0.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64200</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64800</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65100</td>\n",
       "      <td>0.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65400</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65700</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66300</td>\n",
       "      <td>0.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66600</td>\n",
       "      <td>0.508800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=66842, training_loss=0.5907745062279418, metrics={'train_runtime': 40722.9368, 'train_samples_per_second': 4.924, 'train_steps_per_second': 1.641, 'total_flos': 5.47418164909056e+17, 'train_loss': 0.5907745062279418, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = maskTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=3,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        output_dir=\"outputs\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=300,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate=3e-4,\n",
    "        resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved/translation/1.3b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# class StoppingCriteriaSub(StoppingCriteria):\n",
    "#     def __init__(self, stops = [], encounters=1):\n",
    "#         super().__init__()\n",
    "#         self.stops = [stop for stop in stops]\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "#         for stop in self.stops:\n",
    "#             if torch.all((stop == input_ids[0][-len[stop]:])).item():\n",
    "#                 return True\n",
    "            \n",
    "#         return False\n",
    "    \n",
    "# stop_words = [\"</끝>\"]\n",
    "# stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "# stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_words_ids)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words = [\"</끝>\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(lan=\"en\", x=\"\"):\n",
    "    if (lan == \"ko\"):\n",
    "        prompt = f\"### 한국어: {x}</끝>\\n### 영어:\"\n",
    "    else:\n",
    "        prompt = f\"### 영어: {x}</끝>\\n### 한국어:\"\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ),\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.001,\n",
    "        no_repeat_ngram_size=10,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]).replace(prompt+\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 학생입니다.</끝>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(lan=\"ko\", x=\"i am a student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(30080, 2048)\n",
      "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x GPTNeoXLayer(\n",
      "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (attention): GPTNeoXAttention(\n",
      "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "              (query_key_value): Linear4bit(\n",
      "                in_features=2048, out_features=6144, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (mlp): GPTNeoXMLP(\n",
      "              (dense_h_to_4h): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "              (dense_4h_to_h): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "              (act): GELUActivation()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9(translateDnD)",
   "language": "python",
   "name": "translatednd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
