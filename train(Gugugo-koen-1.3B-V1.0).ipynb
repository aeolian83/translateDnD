{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/aeolian83/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"aeolian83/DnD_translate_v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'english', 'korean'],\n",
       "        num_rows: 115397\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레익 게스라스\n",
      "\n",
      "Rayic Gethras\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "print(data['train'][20000]['korean'])\n",
    "print()\n",
    "print(data['train'][20000]['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 형태를 prompt에 맞추기\n",
    "# korean과 english로 나뉜 데이터를 text로 통합\n",
    "# datasets의 map함수 사용법 https://huggingface.co/docs/datasets/v2.14.4/en/process#map\n",
    "\n",
    "def makedata(x):\n",
    "    return {'text': f\"### 영어: {x['english']}</끝>\\n### 한국어: {x['korean']}</끝>\"}\n",
    "\n",
    "data = data.map(makedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 100,\n",
       " 'english': \"Stop a second there, young 'un, 'cuz I heard what you gone and done for old Brun. It made me ashamed that I didn't try to help him myself. Good to know there's still someone willin' ta go the extra mile, even for a stranger. I want you to have this heirloom of mine because of what you did. Now my heirs didn't loom so big, but you ought to get more use out o' it than my mantle does. Folks like you make a jaded old man think there's still decent people out there.\",\n",
       " 'korean': '잠깐만 젊은이, 자네가 브룬을 여러모로 힘써주었다는 얘기를 들었다네. 난 그 녀석을 직접 도우려고 시도조차 하지 않았지, 부끄러울 따름이네. 아직까지 낯선 사람을 선뜻 도와주는 사람들이 남아있다는 게 기쁘군. 나는 그 보답으로 우리 집안의 가보를 주고 싶네. 별 볼 일 없는 가보라도, 나보다는 자네에게 더 쓸모 있을 것 같아. 자네 같은 친구 덕에 아직도 세상엔 착한 사람들이 남아있다는 걸 이 노인네도 실감했다네.',\n",
       " 'text': \"### 영어: Stop a second there, young 'un, 'cuz I heard what you gone and done for old Brun. It made me ashamed that I didn't try to help him myself. Good to know there's still someone willin' ta go the extra mile, even for a stranger. I want you to have this heirloom of mine because of what you did. Now my heirs didn't loom so big, but you ought to get more use out o' it than my mantle does. Folks like you make a jaded old man think there's still decent people out there.</끝>\\n### 한국어: 잠깐만 젊은이, 자네가 브룬을 여러모로 힘써주었다는 얘기를 들었다네. 난 그 녀석을 직접 도우려고 시도조차 하지 않았지, 부끄러울 따름이네. 아직까지 낯선 사람을 선뜻 도와주는 사람들이 남아있다는 게 기쁘군. 나는 그 보답으로 우리 집안의 가보를 주고 싶네. 별 볼 일 없는 가보라도, 나보다는 자네에게 더 쓸모 있을 것 같아. 자네 같은 친구 덕에 아직도 세상엔 착한 사람들이 남아있다는 걸 이 노인네도 실감했다네.</끝>\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 50000,\n",
       " 'english': 'Good. Stay here. I must find Lady Galvena and, if the Lady is cooperative, Claire.',\n",
       " 'korean': '좋아. 여기 머물러. 갈베나 여사와, 만약 그녀가 협조적이라면, 클레어를 반드시 찾겠어.',\n",
       " 'text': '### 영어: Good. Stay here. I must find Lady Galvena and, if the Lady is cooperative, Claire.</끝>\\n### 한국어: 좋아. 여기 머물러. 갈베나 여사와, 만약 그녀가 협조적이라면, 클레어를 반드시 찾겠어.</끝>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"squarelike/Gugugo-koen-1.3B-V1.0\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'english', 'korean', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 115397\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 영어: Rayic Gethras</끝>\\n### 한국어: 레익 게스라스</끝>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][20000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 90000,\n",
       " 'english': 'And why not? We are a force to be reckoned with, a power matched by few. ',\n",
       " 'korean': '그러면 안 될 것도 있나? 우리는 누구도 무시할 수 없는 사람이다, 우리 힘에 대적할 자는 거의 없다.',\n",
       " 'text': '### 영어: And why not? We are a force to be reckoned with, a power matched by few. </끝>\\n### 한국어: 그러면 안 될 것도 있나? 우리는 누구도 무시할 수 없는 사람이다, 우리 힘에 대적할 자는 거의 없다.</끝>',\n",
       " 'input_ids': [6,\n",
       "  6,\n",
       "  6,\n",
       "  3029,\n",
       "  29,\n",
       "  1300,\n",
       "  17130,\n",
       "  9883,\n",
       "  23153,\n",
       "  10590,\n",
       "  7908,\n",
       "  34,\n",
       "  3792,\n",
       "  72,\n",
       "  224,\n",
       "  17081,\n",
       "  12063,\n",
       "  16092,\n",
       "  9138,\n",
       "  13384,\n",
       "  8517,\n",
       "  72,\n",
       "  21924,\n",
       "  70,\n",
       "  78,\n",
       "  2600,\n",
       "  6057,\n",
       "  25614,\n",
       "  15,\n",
       "  12063,\n",
       "  8669,\n",
       "  28559,\n",
       "  7789,\n",
       "  3432,\n",
       "  29537,\n",
       "  71,\n",
       "  8517,\n",
       "  92,\n",
       "  8759,\n",
       "  11594,\n",
       "  17,\n",
       "  422,\n",
       "  18,\n",
       "  5568,\n",
       "  33,\n",
       "  202,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  8611,\n",
       "  29,\n",
       "  4275,\n",
       "  596,\n",
       "  1180,\n",
       "  388,\n",
       "  309,\n",
       "  327,\n",
       "  392,\n",
       "  34,\n",
       "  848,\n",
       "  272,\n",
       "  2041,\n",
       "  309,\n",
       "  4527,\n",
       "  482,\n",
       "  365,\n",
       "  550,\n",
       "  272,\n",
       "  790,\n",
       "  270,\n",
       "  267,\n",
       "  15,\n",
       "  848,\n",
       "  1156,\n",
       "  274,\n",
       "  351,\n",
       "  407,\n",
       "  482,\n",
       "  454,\n",
       "  272,\n",
       "  2440,\n",
       "  550,\n",
       "  267,\n",
       "  17,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][90000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rayic Gethras'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][20000][\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 영어\n",
      " 한국어\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(3029))\n",
    "print(tokenizer.decode(8611))\n",
    "print(tokenizer.decode(29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainble%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 729403392 || trainble%: 0.21563705588032142\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenNum_korean = 8611\n",
    "tokenNum_english = 3029\n",
    "tokenNum_colon = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class maskTrainer(Trainer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # print(inputs['labels'][1])\n",
    "    for x in range(len(inputs['labels'])):\n",
    "      if (inputs['labels'][x][3] == tokenNum_korean):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_english).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      elif (inputs['labels'][x][3] == tokenNum_english):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_korean).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      inputs['labels'][x][:maskindex[0]+2] = -100\n",
    "\n",
    "    # print(inputs['labels'][1])\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    return (loss,outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3390' max='346191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3390/346191 06:46 < 11:25:15, 8.34 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.388900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = maskTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        output_dir=\"outputs\",\n",
    "        save_total_limit=5,\n",
    "        logging_steps=300,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate=3e-4,\n",
    "        resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qlora/translation/Gugugo_for_DnD_v0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# class StoppingCriteriaSub(StoppingCriteria):\n",
    "#     def __init__(self, stops = [], encounters=1):\n",
    "#         super().__init__()\n",
    "#         self.stops = [stop for stop in stops]\n",
    "\n",
    "#     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "#         for stop in self.stops:\n",
    "#             if torch.all((stop == input_ids[0][-len[stop]:])).item():\n",
    "#                 return True\n",
    "            \n",
    "#         return False\n",
    "    \n",
    "# stop_words = [\"</끝>\"]\n",
    "# stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "# stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = stop_words_ids)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "stop_words = [\"</끝>\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(lan=\"en\", x=\"\"):\n",
    "    if (lan == \"ko\"):\n",
    "        prompt = f\"### 한국어: {x}</끝>\\n### 영어:\"\n",
    "    else:\n",
    "        prompt = f\"### 영어: {x}</끝>\\n### 한국어:\"\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ),\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.001,\n",
    "        no_repeat_ngram_size=10,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    return tokenizer.decode(gened[0]).replace(prompt+\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(lan=\"en\", x=\"i am a student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(lan=\"en\", x=\"You remind me of that mouthy gnome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9(translateDnD)",
   "language": "python",
   "name": "translatednd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
