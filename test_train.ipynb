{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squarelike/sharegpt_deepl_ko_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬에 사용된 진리 값과 연결은 실제 개념에 기반하지 않기 때문에 이 특정 비결정적 행렬로 정확하게 모델링할 수 있는 실제 상황을 찾기는 어렵습니다.\n",
      "\n",
      "일반적으로 비결정적 행렬 의미론은 불확실성 또는 여러 가지 가능한 결과가 있는 상황을 모델링하는 데 사용할 수 있습니다. 예를 들어 한 사람이 파티에 갈지 여부를 결정하는 상황을 생각해 보겠습니다. 이 사람이 취할 수 있는 행동에는 파티에 가거나 집에 머무르는 두 가지가 있습니다. 이 사람은 어떤 행동을 취해야 할지 불확실할 수 있으므로 \"파티에 가자\"라는 명제의 진리 값은 2(가능하지만 확실하지 않음을 나타냄)이고 \"집에 머물러라\"라는 명제의 진리 값은 3(가능하지만 확실하지 않음을 나타냄)일 수 있습니다.\n",
      "\n",
      "그러나 이는 가능한 한 가지 예일 뿐이며 상황의 구체적인 세부 사항은 비결정적 행렬에 사용된 진리 값과 연결어의 특정 해석에 따라 달라질 수 있습니다.\n",
      "\n",
      "도움이 되셨기를 바랍니다! 추가 질문이 있으시면 언제든지 알려주시기 바랍니다.\n",
      "\n",
      "It is difficult to find a real-world situation that would be accurately modeled by this specific non-deterministic matrix, as the truth-values and connectives used in the matrix are not based on any real-world concepts.\n",
      "\n",
      "In general, non-deterministic matrix semantics can be used to model situations where there is uncertainty or multiple possible outcomes. For example, consider a situation where a person is deciding whether or not to go to a party. There are two possible actions they can take: go to the party or stay home. The person may be uncertain about which action to take, and so the truth-value of the proposition \"go to the party\" may be 2 (indicating that it is possible but not certain) while the truth-value of the proposition \"stay home\" may be 3 (indicating that it is possible but not certain).\n",
      "\n",
      "However, this is just one possible example and the specific details of the situation would depend on the particular interpretation of the truth-values and connectives used in the non-deterministic matrix.\n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "print(data['train'][20000]['korean'])\n",
    "print()\n",
    "print(data['train'][20000]['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 형태를 prompt에 맞추기\n",
    "# korean과 english로 나뉜 데이터를 text로 통합\n",
    "# datasets의 map함수 사용법 https://huggingface.co/docs/datasets/v2.14.4/en/process#map\n",
    "\n",
    "def makedata(x):\n",
    "    return {'text': f\"### 영어: {x['english']}</끝>\\n### 한국어: {x['korean']}</끝>\"}\n",
    "\n",
    "data = data.map(makedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 100,\n",
       " 'korean': \"회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 '선택 및 마스크' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 '색조/채도' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.\",\n",
       " 'english': 'If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.',\n",
       " 'text': '### 영어: If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.</끝>\\n### 한국어: 회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 \\'선택 및 마스크\\' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 \\'색조/채도\\' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.</끝>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14893d12a4304aebbb8259a39f37007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>',\n",
       " 'input_ids': [6,\n",
       "  6,\n",
       "  6,\n",
       "  3029,\n",
       "  29,\n",
       "  1795,\n",
       "  17291,\n",
       "  72,\n",
       "  224,\n",
       "  17081,\n",
       "  12063,\n",
       "  8759,\n",
       "  11594,\n",
       "  9883,\n",
       "  7743,\n",
       "  86,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  7789,\n",
       "  26618,\n",
       "  22253,\n",
       "  5941,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  29,\n",
       "  202,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9244,\n",
       "  22757,\n",
       "  3883,\n",
       "  29650,\n",
       "  10903,\n",
       "  28078,\n",
       "  7789,\n",
       "  20705,\n",
       "  11518,\n",
       "  4454,\n",
       "  14445,\n",
       "  2479,\n",
       "  5862,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  21924,\n",
       "  7993,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  202,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  224,\n",
       "  3328,\n",
       "  86,\n",
       "  82,\n",
       "  224,\n",
       "  29598,\n",
       "  3628,\n",
       "  8812,\n",
       "  22757,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  13652,\n",
       "  12063,\n",
       "  89,\n",
       "  82,\n",
       "  8614,\n",
       "  224,\n",
       "  10414,\n",
       "  89,\n",
       "  4541,\n",
       "  13384,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  3432,\n",
       "  3354,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  25614,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  2218,\n",
       "  2479,\n",
       "  12131,\n",
       "  92,\n",
       "  15,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9059,\n",
       "  26477,\n",
       "  12509,\n",
       "  12063,\n",
       "  224,\n",
       "  4569,\n",
       "  79,\n",
       "  21473,\n",
       "  8759,\n",
       "  7094,\n",
       "  29828,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  8945,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  28428,\n",
       "  13652,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  224,\n",
       "  21840,\n",
       "  2423,\n",
       "  3864,\n",
       "  7948,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  73,\n",
       "  7094,\n",
       "  29828,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  21924,\n",
       "  87,\n",
       "  9347,\n",
       "  81,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  202,\n",
       "  18,\n",
       "  18,\n",
       "  23375,\n",
       "  3354,\n",
       "  224,\n",
       "  29598,\n",
       "  29650,\n",
       "  14445,\n",
       "  20904,\n",
       "  72,\n",
       "  22253,\n",
       "  5941,\n",
       "  29,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33,\n",
       "  202,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  8611,\n",
       "  29,\n",
       "  307,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  365,\n",
       "  327,\n",
       "  272,\n",
       "  1870,\n",
       "  1384,\n",
       "  1891,\n",
       "  270,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  20,\n",
       "  17,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  954,\n",
       "  1553,\n",
       "  1033,\n",
       "  286,\n",
       "  11567,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  15407,\n",
       "  21396,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  1203,\n",
       "  13473,\n",
       "  409,\n",
       "  998,\n",
       "  417,\n",
       "  276,\n",
       "  2129,\n",
       "  283,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  286,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  2477,\n",
       "  482,\n",
       "  1062,\n",
       "  293,\n",
       "  550,\n",
       "  954,\n",
       "  823,\n",
       "  2248,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  661,\n",
       "  417,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  2086,\n",
       "  339,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  20637,\n",
       "  4296,\n",
       "  1979,\n",
       "  284,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  316,\n",
       "  746,\n",
       "  16776,\n",
       "  847,\n",
       "  419,\n",
       "  301,\n",
       "  3607,\n",
       "  482,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  953,\n",
       "  419,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  224,\n",
       "  8924,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  348,\n",
       "  3962,\n",
       "  2489,\n",
       "  485,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  96,\n",
       "  18,\n",
       "  18,\n",
       "  1462,\n",
       "  359,\n",
       "  2192,\n",
       "  1203,\n",
       "  284,\n",
       "  1934,\n",
       "  29,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainble%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 729403392 || trainble%: 0.21563705588032142\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenNum_korean = 8611\n",
    "tokenNum_english = 3029\n",
    "tokenNum_colon = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class maskTrainer(Trainer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # print(inputs['labels'][1])\n",
    "    for x in range(len(inputs['labels'])):\n",
    "      if (inputs['labels'][x][3] == tokenNum_korean):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_english).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      elif (inputs['labels'][x][3] == tokenNum_english):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_korean).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      inputs['labels'][x][:maskindex[0]+2] = -100\n",
    "\n",
    "    # print(inputs['labels'][1])\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    return (loss,outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeolian83/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15218' max='25066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15218/25066 7:58:46 < 5:09:52, 0.53 it/s, Epoch 0.61/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.932800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.810100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.715700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.579100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.555900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.67 GiB total capacity; 17.66 GiB already allocated; 1.71 GiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m maskTrainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/accelerate/accelerator.py:1916\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/utils/checkpoint.py:157\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    159\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.48 GiB (GPU 0; 23.67 GiB total capacity; 17.66 GiB already allocated; 1.71 GiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = maskTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        output_dir=\"outputs\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=300,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate=3e-4,\n",
    "        resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9(translateDnD)",
   "language": "python",
   "name": "translatednd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
