{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"squarelike/sharegpt_deepl_ko_translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬에 사용된 진리 값과 연결은 실제 개념에 기반하지 않기 때문에 이 특정 비결정적 행렬로 정확하게 모델링할 수 있는 실제 상황을 찾기는 어렵습니다.\n",
      "\n",
      "일반적으로 비결정적 행렬 의미론은 불확실성 또는 여러 가지 가능한 결과가 있는 상황을 모델링하는 데 사용할 수 있습니다. 예를 들어 한 사람이 파티에 갈지 여부를 결정하는 상황을 생각해 보겠습니다. 이 사람이 취할 수 있는 행동에는 파티에 가거나 집에 머무르는 두 가지가 있습니다. 이 사람은 어떤 행동을 취해야 할지 불확실할 수 있으므로 \"파티에 가자\"라는 명제의 진리 값은 2(가능하지만 확실하지 않음을 나타냄)이고 \"집에 머물러라\"라는 명제의 진리 값은 3(가능하지만 확실하지 않음을 나타냄)일 수 있습니다.\n",
      "\n",
      "그러나 이는 가능한 한 가지 예일 뿐이며 상황의 구체적인 세부 사항은 비결정적 행렬에 사용된 진리 값과 연결어의 특정 해석에 따라 달라질 수 있습니다.\n",
      "\n",
      "도움이 되셨기를 바랍니다! 추가 질문이 있으시면 언제든지 알려주시기 바랍니다.\n",
      "\n",
      "It is difficult to find a real-world situation that would be accurately modeled by this specific non-deterministic matrix, as the truth-values and connectives used in the matrix are not based on any real-world concepts.\n",
      "\n",
      "In general, non-deterministic matrix semantics can be used to model situations where there is uncertainty or multiple possible outcomes. For example, consider a situation where a person is deciding whether or not to go to a party. There are two possible actions they can take: go to the party or stay home. The person may be uncertain about which action to take, and so the truth-value of the proposition \"go to the party\" may be 2 (indicating that it is possible but not certain) while the truth-value of the proposition \"stay home\" may be 3 (indicating that it is possible but not certain).\n",
      "\n",
      "However, this is just one possible example and the specific details of the situation would depend on the particular interpretation of the truth-values and connectives used in the non-deterministic matrix.\n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "print(data['train'][20000]['korean'])\n",
    "print()\n",
    "print(data['train'][20000]['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 형태를 prompt에 맞추기\n",
    "# korean과 english로 나뉜 데이터를 text로 통합\n",
    "# datasets의 map함수 사용법 https://huggingface.co/docs/datasets/v2.14.4/en/process#map\n",
    "\n",
    "def makedata(x):\n",
    "    return {'text': f\"### 영어: {x['english']}</끝>\\n### 한국어: {x['korean']}</끝>\"}\n",
    "\n",
    "data = data.map(makedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 100,\n",
       " 'korean': \"회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 '선택 및 마스크' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 '색조/채도' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.\",\n",
       " 'english': 'If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.',\n",
       " 'text': '### 영어: If you want to create an image with a grayscale background and a colored subject, you can use photo editing software to selectively adjust the colors in the image.\\n\\nIn Adobe Photoshop, for example, you can use the \"Select and Mask\" tool to isolate the subject of the photograph and then apply a grayscale effect to the rest of the image. Alternatively, you can use the \"Hue/Saturation\" adjustment layer to selectively adjust the colors in the image, desaturating the background while leaving the subject in color.\\n\\nOther photo editing software may have similar tools or options for selectively adjusting the colors in an image. You can also use these tools to create other color effects, such as isolating a specific color range in the image and adjusting its saturation or hue.</끝>\\n### 한국어: 회색조 배경과 컬러 피사체가 있는 이미지를 만들려면 사진 편집 소프트웨어를 사용하여 이미지의 색상을 선택적으로 조정할 수 있습니다.\\n\\n예를 들어 Adobe Photoshop에서는 \\'선택 및 마스크\\' 도구를 사용하여 사진의 피사체를 분리한 다음 나머지 이미지에 그레이 스케일 효과를 적용할 수 있습니다. 또는 \\'색조/채도\\' 조정 레이어를 사용하여 이미지의 색상을 선택적으로 조정하여 피사체는 컬러로 유지하면서 배경의 채도를 낮출 수 있습니다.\\n\\n다른 사진 편집 소프트웨어에도 이미지의 색상을 선택적으로 조정할 수 있는 유사한 도구 또는 옵션이 있을 수 있습니다. 이러한 도구를 사용하여 이미지에서 특정 색상 범위를 분리하고 채도 또는 색조를 조정하는 등 다른 색상 효과를 만들 수도 있습니다.</끝>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad54c3646b5b47f69abe7c10be8e5ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['num', 'korean', 'english', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 200524\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 150000,\n",
       " 'korean': '이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```',\n",
       " 'english': 'There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```',\n",
       " 'text': '### 영어: There are a few ways you could make this code more readable:\\n\\n1. You could split it into multiple lines to make it easier to read:\\n```\\ndata-test={\\n  dataTestAttribute + \"-\" +\\n  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")\\n}\\n```\\n1. You could also use template strings to make it more readable and avoid having to concatenate strings with `+`:\\n```\\ndata-test={`${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}\\n```\\n1. Finally, you could define a helper function to make the code more concise and easier to understand:\\n```\\nfunction formatDataTestAttribute(dataTestAttribute, col) {\\n  return `${dataTestAttribute}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;\\n}\\n\\n// Then use it like this:\\ndata-test={formatDataTestAttribute(dataTestAttribute, col)}\\n```</끝>\\n### 한국어: 이 코드를 더 읽기 쉽게 만들 수 있는 몇 가지 방법이 있습니다:1. 읽기 쉽도록 여러 줄로 나눌 수 있습니다:```data-test={  dataTestAttribute + \"-\" +  col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}```1. 템플릿 문자열을 사용하여 가독성을 높이고 `+`로 문자열을 연결할 필요가 없도록 할 수도 있습니다:```data-test={`${데이터테스트속성}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`}```1. 마지막으로 코드를 더 간결하고 이해하기 쉽게 만들기 위해 도우미 함수를 정의할 수 있습니다:```함수 formatDataTestAttribute(dataTestAttribute, col) {  반환 `${데이터테스트어트리뷰트}-${col.label.toLowerCase().replace(\" \", \"\").replace(\".\", \"\")}`;}// 다음과 같이 사용하세요:data-test={formatDataTestAttribute(dataTestAttribute, col)}```</끝>',\n",
       " 'input_ids': [6,\n",
       "  6,\n",
       "  6,\n",
       "  3029,\n",
       "  29,\n",
       "  1795,\n",
       "  17291,\n",
       "  72,\n",
       "  224,\n",
       "  17081,\n",
       "  12063,\n",
       "  8759,\n",
       "  11594,\n",
       "  9883,\n",
       "  7743,\n",
       "  86,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  7789,\n",
       "  26618,\n",
       "  22253,\n",
       "  5941,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  29,\n",
       "  202,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9244,\n",
       "  22757,\n",
       "  3883,\n",
       "  29650,\n",
       "  10903,\n",
       "  28078,\n",
       "  7789,\n",
       "  20705,\n",
       "  11518,\n",
       "  4454,\n",
       "  14445,\n",
       "  2479,\n",
       "  5862,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  21924,\n",
       "  7993,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  202,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  26349,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  224,\n",
       "  3328,\n",
       "  86,\n",
       "  82,\n",
       "  224,\n",
       "  29598,\n",
       "  3628,\n",
       "  8812,\n",
       "  22757,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  29650,\n",
       "  7789,\n",
       "  10101,\n",
       "  21924,\n",
       "  7993,\n",
       "  21762,\n",
       "  13652,\n",
       "  12063,\n",
       "  89,\n",
       "  82,\n",
       "  8614,\n",
       "  224,\n",
       "  10414,\n",
       "  89,\n",
       "  4541,\n",
       "  13384,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  3432,\n",
       "  3354,\n",
       "  14237,\n",
       "  19800,\n",
       "  85,\n",
       "  4541,\n",
       "  86,\n",
       "  25614,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  20,\n",
       "  17,\n",
       "  2218,\n",
       "  2479,\n",
       "  12131,\n",
       "  92,\n",
       "  15,\n",
       "  26661,\n",
       "  224,\n",
       "  3855,\n",
       "  9861,\n",
       "  71,\n",
       "  9059,\n",
       "  26477,\n",
       "  12509,\n",
       "  12063,\n",
       "  224,\n",
       "  4569,\n",
       "  79,\n",
       "  21473,\n",
       "  8759,\n",
       "  7094,\n",
       "  29828,\n",
       "  13384,\n",
       "  7789,\n",
       "  26618,\n",
       "  8945,\n",
       "  224,\n",
       "  3855,\n",
       "  71,\n",
       "  72,\n",
       "  7789,\n",
       "  10101,\n",
       "  11313,\n",
       "  2600,\n",
       "  70,\n",
       "  28428,\n",
       "  13652,\n",
       "  6341,\n",
       "  7245,\n",
       "  28100,\n",
       "  13384,\n",
       "  224,\n",
       "  21840,\n",
       "  2423,\n",
       "  3864,\n",
       "  7948,\n",
       "  29,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  202,\n",
       "  73,\n",
       "  7094,\n",
       "  29828,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  202,\n",
       "  224,\n",
       "  21924,\n",
       "  87,\n",
       "  9347,\n",
       "  81,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  202,\n",
       "  96,\n",
       "  202,\n",
       "  202,\n",
       "  18,\n",
       "  18,\n",
       "  23375,\n",
       "  3354,\n",
       "  224,\n",
       "  29598,\n",
       "  29650,\n",
       "  14445,\n",
       "  20904,\n",
       "  72,\n",
       "  22253,\n",
       "  5941,\n",
       "  29,\n",
       "  202,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  202,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33,\n",
       "  202,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  8611,\n",
       "  29,\n",
       "  307,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  365,\n",
       "  327,\n",
       "  272,\n",
       "  1870,\n",
       "  1384,\n",
       "  1891,\n",
       "  270,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  20,\n",
       "  17,\n",
       "  2327,\n",
       "  316,\n",
       "  1790,\n",
       "  954,\n",
       "  1553,\n",
       "  1033,\n",
       "  286,\n",
       "  11567,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  224,\n",
       "  9059,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  18008,\n",
       "  490,\n",
       "  16,\n",
       "  5,\n",
       "  18008,\n",
       "  224,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  15407,\n",
       "  21396,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  1203,\n",
       "  13473,\n",
       "  409,\n",
       "  998,\n",
       "  417,\n",
       "  276,\n",
       "  2129,\n",
       "  283,\n",
       "  3670,\n",
       "  14,\n",
       "  67,\n",
       "  286,\n",
       "  4529,\n",
       "  1233,\n",
       "  276,\n",
       "  2477,\n",
       "  482,\n",
       "  1062,\n",
       "  293,\n",
       "  550,\n",
       "  954,\n",
       "  823,\n",
       "  2248,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  67,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  661,\n",
       "  417,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  20,\n",
       "  17,\n",
       "  2086,\n",
       "  339,\n",
       "  10746,\n",
       "  301,\n",
       "  715,\n",
       "  20637,\n",
       "  4296,\n",
       "  1979,\n",
       "  284,\n",
       "  316,\n",
       "  1790,\n",
       "  379,\n",
       "  1075,\n",
       "  316,\n",
       "  746,\n",
       "  16776,\n",
       "  847,\n",
       "  419,\n",
       "  301,\n",
       "  3607,\n",
       "  482,\n",
       "  365,\n",
       "  327,\n",
       "  827,\n",
       "  29,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  953,\n",
       "  419,\n",
       "  16092,\n",
       "  80,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  224,\n",
       "  94,\n",
       "  224,\n",
       "  8924,\n",
       "  3670,\n",
       "  7,\n",
       "  94,\n",
       "  4733,\n",
       "  12614,\n",
       "  348,\n",
       "  3962,\n",
       "  2489,\n",
       "  485,\n",
       "  96,\n",
       "  16,\n",
       "  7,\n",
       "  94,\n",
       "  3855,\n",
       "  79,\n",
       "  17,\n",
       "  79,\n",
       "  10144,\n",
       "  6063,\n",
       "  17,\n",
       "  28078,\n",
       "  47,\n",
       "  28559,\n",
       "  38,\n",
       "  25356,\n",
       "  11,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  490,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  17,\n",
       "  4065,\n",
       "  22757,\n",
       "  16561,\n",
       "  11,\n",
       "  5,\n",
       "  9098,\n",
       "  15,\n",
       "  490,\n",
       "  5,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  30,\n",
       "  96,\n",
       "  18,\n",
       "  18,\n",
       "  1462,\n",
       "  359,\n",
       "  2192,\n",
       "  1203,\n",
       "  284,\n",
       "  1934,\n",
       "  29,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  16,\n",
       "  87,\n",
       "  11273,\n",
       "  32,\n",
       "  94,\n",
       "  24998,\n",
       "  3432,\n",
       "  39,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  11,\n",
       "  71,\n",
       "  3432,\n",
       "  68,\n",
       "  55,\n",
       "  11273,\n",
       "  36,\n",
       "  87,\n",
       "  87,\n",
       "  13274,\n",
       "  69,\n",
       "  8846,\n",
       "  72,\n",
       "  15,\n",
       "  224,\n",
       "  3855,\n",
       "  79,\n",
       "  12,\n",
       "  96,\n",
       "  67,\n",
       "  67,\n",
       "  67,\n",
       "  31,\n",
       "  18,\n",
       "  5568,\n",
       "  33],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainble%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 729403392 || trainble%: 0.21563705588032142\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenNum_korean = 8611\n",
    "tokenNum_english = 3029\n",
    "tokenNum_colon = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class maskTrainer(Trainer):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # print(inputs['labels'][1])\n",
    "    for x in range(len(inputs['labels'])):\n",
    "      if (inputs['labels'][x][3] == tokenNum_korean):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_english).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      elif (inputs['labels'][x][3] == tokenNum_english):\n",
    "          maskindex = (inputs['labels'][x]==tokenNum_korean).nonzero()[:, 0]\n",
    "          temp = 0\n",
    "          for i, index in enumerate(maskindex):\n",
    "            if (inputs['labels'][x][index+1] != tokenNum_colon):\n",
    "              maskindex = np.delete(maskindex.cpu(), i-temp)\n",
    "              temp += 1\n",
    "\n",
    "      inputs['labels'][x][:maskindex[0]+2] = -100\n",
    "\n",
    "    # print(inputs['labels'][1])\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    loss = outputs['loss']\n",
    "\n",
    "    return (loss,outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='12533' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   31/12533 01:57 < 14:07:25, 0.25 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m maskTrainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m     train_dataset\u001b[39m=\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     data_collator\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/transformers/trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/accelerate/accelerator.py:1916\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/translateDnD/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = maskTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=True,\n",
    "        output_dir=\"outputs\",\n",
    "        save_total_limit=3,\n",
    "        logging_steps=300,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate=3e-4,\n",
    "        resume_from_checkpoint=True,\n",
    "        lr_scheduler_type= \"cosine\",\n",
    "        #optim=\"paged_adamw_8bit\"\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translateDnD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
